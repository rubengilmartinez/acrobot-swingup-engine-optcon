% Template for OPTCON-RL course projects - Acrobot Assignment
\documentclass[a4paper,11pt,oneside]{book}

% Use UTF-8 for easy typing of tildes like 'é' and 'ñ'
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb,color}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}


\begin{document}
	\pagestyle{myheadings}
	
	%%%%%%%%%%% Cover %%%%%%%%%%%
	\thispagestyle{empty}                                        
	\begin{center}                                                         
		\vspace{5mm}
		{\LARGE UNIVERSIT\`A DI BOLOGNA} \\                       
		\vspace{5mm}
	\end{center}
	\begin{center}
		% Ensure this file exists in a 'figs' folder
		\includegraphics[scale=.27]{figs/logo_unibo}
	\end{center}
	\begin{center}
		\vspace{5mm}
		{\LARGE School of Engineering} \\
		\vspace{3mm}
		{\Large Master Degree in Automation Engineering} \\
		\vspace{20mm}
		{\LARGE Optimal Control and Reinforcement Learning} \\
		\vspace{5mm}{\Large\textbf{OPTIMAL CONTROL OF A GYMNAST ROBOT}}                  
		\vspace{15mm}
	\end{center}
	
	\begin{center}                                                                                
		{\large Professor: \textbf{Giuseppe Notarstefano}} \\        
		\vspace{13mm}
	\end{center}
	
	\begin{center}
		{\large Students:}\\
	\end{center}
	
	\begin{center}
		{\textbf{Rubén Gil Martínez}}\\
	\end{center}
	
	
	
	\begin{center}
		{\textbf{Guillermo López Pérez}}\\
	\end{center}
	
	
	
	\begin{center}
		\vfill
		{\large Academic year 2025/2026} \\
	\end{center}
	
	\newpage
	\thispagestyle{empty}
	
	%%%%%%%%%%% Abstract %%%%%%%%%%%%
	\chapter*{Abstract}
	\thispagestyle{empty}
	This project focuses on the optimal control of a planar gymnast robot, modeled as a double pendulum with torque applied only at the hip (Acrobot). We implement trajectory generation using Newton-like algorithms and tracking via LQR and MPC techniques.
	
	\newpage
	%%%%%%%%%% Lists %%%%%%%%%%
	\tableofcontents 
	\thispagestyle{empty}
	\newpage
	\listoffigures 
	\thispagestyle{empty}
	\newpage
	
	%%%%%%%%%% Introduction %%%%%%%%%%
	\chapter*{Introduction}
	\addcontentsline{toc}{chapter}{Introduction}
	The Acrobot, a planar gymnast robot, serves as a classic benchmark for underactuated mechanical systems. Comprising two links with torque applied solely at the hip joint, it presents a significant control challenge due to its highly non-linear dynamics and the lack of direct actuation at the first joint. 

    The primary objective of this project is to design and implement an optimal control framework capable of driving the system from its stable downward hanging state to the unstable upright equilibrium through a coordinated swing-up maneuver. This involves several integrated technical phases:
    
    \begin{itemize}
        \item \textbf{Task 0: Modeling and Discretization:} Derivation of the system's equations of motion and implementation of the 4th-order Runge-Kutta (RK4) scheme for high-fidelity numeric integration.
        \item \textbf{Task 1 \& 2: Trajectory Generation:} Development of an Iterative Linear Quadratic Regulator (iLQR) algorithm. We explore the impact of reference choice by comparing a naive step reference with a physically-motivated smooth reference that leverages the system's natural frequency to minimize control effort.
        \item \textbf{Task 3: Feedback Tracking via LQR:} Implementation of a Time-Varying LQR controller designed to follow the optimal trajectory while rejecting disturbances and ensuring local stability.
        \item \textbf{Task 4: Predictive Control via MPC:} Implementation of Model Predictive Control (MPC) to enhance tracking performance and robustness by explicitly considering future system behavior.
    \end{itemize}

    By combining trajectory optimization with robust feedback control, we aim to achieve a seamless transitions between equilibria, demonstrating the efficacy of modern optimal control techniques in handling complex robotic dynamics.











	%%%%%%%%%% Tasks %%%%%%%%%%
	
\chapter{Task 0: Problem Setup and Discretization}
\section{System Description}
The Acrobot is a generic planar gymnast robot consisting of two links and two joints. It is a canonical example of an underactuated mechanical system, where control input is only available at the second joint (the "hip"), while the first joint (the "shoulder") is passive.

The state of the system is defined by the generalized coordinates $q = [\theta_1, \theta_2]^\top$, where $\theta_1$ is the angle of the first link with respect to the vertical axis, and $\theta_2$ is the relative angle of the second link with respect to the first. The full state vector is given by $x = [\theta_1, \theta_2, \dot{\theta}_1, \dot{\theta}_2]^\top \in \mathbb{R}^4$. The control input $u = \tau \in \mathbb{R}$ represents the torque applied at the hip joint.

\section{Equations of Motion}
The continuous-time dynamics of the Acrobot are derived using the Euler-Lagrange formalism and can be expressed in the standard manipulator form:
\begin{equation}
    M(q)\ddot{q} + C(q, \dot{q})\dot{q} + F\dot{q} + G(q) = B u
\end{equation}
where:
\begin{itemize}
    \item $M(q) \in \mathbb{R}^{2 \times 2}$ is the symmetric, positive-definite mass matrix.
    \item $C(q, \dot{q}) \in \mathbb{R}^{2 \times 2}$ represents Coriolis and centrifugal forces.
    \item $F \in \mathbb{R}^{2 \times 2}$ is the diagonal matrix of viscous friction coefficients.
    \item $G(q) \in \mathbb{R}^2$ is the gravity vector.
    \item $B = [0, 1]^\top$ is the actuation matrix.
\end{itemize}

For simulation and control design purposes, we solve for the joint accelerations $\ddot{q}$:
\begin{equation}
    \ddot{q} = M(q)^{-1} \left( B u - C(q, \dot{q})\dot{q} - F\dot{q} - G(q) \right)
\end{equation}

\section{Physical Parameters}
The project employs Parameter Set 3, which defines the following physical constants:
\begin{table}[H]
    \centering
    \begin{tabular}{lcl}
        \hline
        Parameter & Symbol & Value \\
        \hline
        Mass of link 1 & $m_1$ & 1.5 kg \\
        Mass of link 2 & $m_2$ & 1.5 kg \\
        Length of link 1 & $l_1$ & 2.0 m \\
        Length of link 2 & $l_2$ & 2.0 m \\
        COM distance link 1 & $l_{c1}$ & 1.0 m \\
        COM distance link 2 & $l_{c2}$ & 1.0 m \\
        Inertia of link 1 & $I_1$ & 2.0 kg$\cdot$m$^2$ \\
        Inertia of link 2 & $I_2$ & 2.0 kg$\cdot$m$^2$ \\
        Gravity & $g$ & 9.81 m/s$^2$ \\
        Viscous friction & $f_1, f_2$ & 1.0 N$\cdot$m$\cdot$s/rad \\
        \hline
    \end{tabular}
    \caption{Acrobot Physical Parameters (Set 3)}
    \label{tab:params}
\end{table}



\section{Discretization via Runge-Kutta 4}
For numerical simulation and discrete-time optimal control, the continuous-time dynamics $\dot{x} = f_c(x, u)$ must be discretized. We employ the 4th-order Runge-Kutta (RK4) integration scheme with a constant sampling period $d_t = 0.01$ s. Given the state at time $t$, the next state $x_{t+1}$ is computed as:
\begin{subequations}
    \begin{align}
        k_1 &= f_c(x_t, u_t) \\
        k_2 &= f_c(x_t + \frac{d_t}{2}k_1, u_t) \\
        k_3 &= f_c(x_t + \frac{d_t}{2}k_2, u_t) \\
        k_4 &= f_c(x_t + d_t k_3, u_t) \\
        x_{t+1} &= x_t + \frac{d_t}{6}(k_1 + 2k_2 + 2k_3 + k_4)
    \end{align}
\end{subequations}
This numeric integration method ensures robust stability and accuracy, facilitating the application of gradient-based optimization algorithms.














\chapter{Task 1 and 2: Trajectory Generation.}
\label{chap:trajectory_generation}

In this chapter, we address the core problem of trajectory generation: finding an optimal control sequence $u(t)$ and state trajectory $x(t)$ that transitions the Acrobot from a stable hanging position to the unstable upright equilibrium \cite{Notarstefano2025Trajectory}. We employ a Newton-type optimal control algorithm (specifically, Differential Dynamic Programming making use of Iterative LQR method) to solve this non-linear optimization problem.

The process is divided into three logical steps: computing the boundary conditions (equilibria), defining a reference trajectory to guide the solver, and executing the iterative optimization loop.




\section{Step 1: Computation of Equilibria}

The first requirement is to rigorously define the start and end points of the maneuver. An equilibrium is defined as a state where the system remains stationary if no external force is applied (or a constant holding torque is maintained). Mathematically, this implies that the acceleration and velocity are zero:
\begin{equation}
    \ddot{q} = 0, \quad \dot{q} = 0 \implies \dot{x} = 0
\end{equation}
We must solve the static equation derived from the dynamics $f(x, u)$:
\begin{equation}
    f(x_{eq}, u_{eq}) = 0
\end{equation}
Given that the gravity vector $G(q)$ and other matrices contain non-linear trigonometric terms ($\sin(\theta_1), \sin(\theta_1 + \theta_2), \sin(\theta_2), \cos(\theta_1), \cos(\theta_2) $), an analytical solution is non-trivial. Therefore, we employ a numerical root-finding algorithm to solve for the configuration angles.

We identified two key equilibria:
\begin{itemize}
    \item \textbf{Stable Equilibrium ($x_{eq1}$):} The "Down" position where the robot hangs vertically under gravity ($x \approx [0, 0, 0, 0]^\top$).
    \item \textbf{Unstable Equilibrium ($x_{eq2}$):} The "Up" position where the robot is balanced perfectly inverted ($x \approx [\pi, 0, 0, 0]^\top$).
\end{itemize}







\section{Step 2: Definition of Reference Curves}

The optimization algorithm requires a reference trajectory $x_{ref}(t)$ to define the cost function. We explored two different approaches to generating this reference, highlighting the impact of physical consistency on solver convergence.

\subsection{Task 1: The Step Reference}
In the first approach, we define a "Step" reference. The time horizon $T$ is split into two halves:
\begin{equation}
    x_{ref}(t) = 
    \begin{cases} 
        x_{eq1} & \text{if } 0 \le t < T/2 \\
        x_{eq2} & \text{if } T/2 \le t \le T 
    \end{cases}
\end{equation}
This reference commands the robot to stay at the bottom for the first half and immediately teleport to the top for the second half.

\textbf{Limitations:} This creates a conflict between the \textit{optimization objective} and the \textit{system physics}. To reach the top, an underactuated robot must swing back and forth ("pump" energy). However, the cost function $J$ penalizes any deviation from zero during the first half. The solver is forced to fight this penalty to discover the necessary swinging motion, often resulting in slower convergence.

\subsection{Task 2: The "Natural" Smooth Reference}
In the second approach, we construct a "Physically Inspired" reference that anticipates the system's dynamics. This reference is composed of two phases:

\begin{enumerate}
    \item \textbf{Phase 1 (Energy Pumping):} We inject sinusoidal oscillations into the reference trajectory. The frequency of these oscillations is tuned to the natural frequency of a single-arm pendulum, approximated by the simplified model $\omega_n \approx \sqrt{g/L_{eff}}$. Where $g$ is the acceleration due to gravity ($g \approx 9.81$ m/s$^2$) and $L_{eff}$ is the effective length of the first part of the pendulum ($L_{eff} \approx L_1$).
    \item \textbf{Phase 2 (Smooth Rise):} We utilize a sigmoid function (logistic curve) to transition smoothly from the oscillating bottom state to the inverted equilibrium, avoiding the infinite derivatives associated with a step change.
\end{enumerate}

\textbf{Advantages for Convergence:} By adding oscillations, we align the cost function with the physics. We effectively signal to the solver: \textit{"It is optimal to swing here."} This reduces the conflict between the objective function and the dynamic constraints, creating a smoother optimization landscape and facilitating faster, more robust convergence.








\section{Step 3: The Optimization Loop (Newton via iLQR)}

With the boundary conditions and reference defined, we implement the Newton-type optimal control algorithm. In every iteration $k$, we locally approximate the problem by solving a finite-horizon \textbf{LQR Affine} problem to find the optimal control deviation, this sequence of solved subproblems forms the Newton-type update. \cite{Notarstefano2025}.

The process consists of three distinct phases. Below, we detail the mathematical formulation and provide a direct mapping to the variables used in our Python implementation.

\subsection{Phase 1: Forward Pass (Nominal Trajectory)}
We simulate the non-linear dynamics using the current control sequence $\bar{u} = 0$ and $\bar{x_0} = x_{eq1}$, here, we obtain this first feasible trajectory via \textbf{Shooting}.
\begin{equation}
    \bar{x}_{t+1} = f(\bar{x}_t, \bar{u}_t)
\end{equation}
\textbf{Code Mapping:}
\begin{itemize}
    \item $\bar{x}_t, \bar{u}_t \rightarrow$ \texttt{x\_traj[i]}, \texttt{u[i]}
    \item $f(\cdot) \rightarrow$ \texttt{discrete\_step\_rk4(x, u, dt)}
\end{itemize}

\subsection{Phase 2: Backward Pass (Solving LQR Affine)}
This phase computes the feedback gains and feedforward corrections by propagating the computation of the Riccati difference equations backwards in time.

\subsubsection*{1. Initialization ($t=T$)}
We initialize the Cost-to-Go vector and matrix.
\begin{equation}
    P_T = Q_T, \quad p_T = Q_T(x_T - x_{ref,T})
\end{equation}
\textbf{Code Mapping:}
\begin{itemize}
    \item $P_T \rightarrow$ \texttt{P = self.Q\_final}
    \item $p_T \rightarrow$ \texttt{p = self.Q\_final @ dx\_terminal}
\end{itemize}

\subsubsection*{2. Linearization and Approximation}
For each step $t$, we compute the local derivatives.
\begin{itemize}
    \item Dynamics ($A_t, B_t$): \texttt{A\_t}, \texttt{B\_t} (via \texttt{get\_derivatives\_fd}).
    \item Taylor Linear Terms ($q_t, r_t$): \texttt{q\_t}, \texttt{r\_t} (via \texttt{Q @ dx}, \texttt{R @ du}).
    \item Taylor Quadratic Terms (Cost Hessians) ($Q_t, R_t$): \texttt{self.Q}, \texttt{self.R}.
\end{itemize}

\subsubsection*{3. The "S" Matrix (auxiliary term)}
To simplify the gain computation, we define the matrix $S_t$, which represents the Hessian of the Action-Value function with respect to the control input $u$.
\begin{equation}
    S_t = R_t + B_t^\top P_{t+1} B_t
\end{equation}
\textbf{Code Mapping:}
\begin{itemize}
    \item $S_t \rightarrow$ \texttt{S\_mat = self.R + B\_t.T @ P @ B\_t}
    \item $S_t^{-1} \rightarrow$ \texttt{S\_inv} (Regularized inverse)
\end{itemize}

\subsubsection*{4. Computing the Gains}
We calculate the optimal \textbf{Feedback Gain} $K_t^*$ and \textbf{Feedforward Correction} $\sigma_t^*$ by minimizing the local quadratic model \cite{Notarstefano2025}:
\begin{align}
    K_t^* &= -S_t^{-1} (B_t^\top P_{t+1} A_t) \\
    \sigma_t^* &= -S_t^{-1} (r_t + B_t^\top p_{t+1})
\end{align}
\textbf{Code Mapping:}
\begin{itemize}
    \item $K_t^* \rightarrow$ \texttt{K\_t} (stored in \texttt{K\_gains})
    \item $\sigma_t^* \rightarrow$ \texttt{sigma\_t} (stored in \texttt{sigma\_vec})
\end{itemize}

\subsubsection*{5. Difference Riccati Equation Update}
Finally, we update the Cost-to-Go parameters ($P_t, p_t$) for the previous time step. In the code, we utilize the computed gains to perform this update efficiently. 
We have to keep in mind that $c_t$, which is a offset term for the linearization, can be set to zero becuase we define the problem in terms of deviations from the nominal trajectory,
thus, the variables that the optimal control problem depends on are $x_t - \bar{x}_t$ and $u_t - \bar{u}_t$.
\begin{align}
    P_t &= Q_t + A_t^\top P_{t+1} A_t - K_t^{*\top} S_t K_t^* \\
    p_t &= q_t + A_t^\top p_{t+1} - K_t^{*\top} S_t \sigma_t^* \\
    c_t &= 0
\end{align}
\textbf{Code Mapping:}
\begin{itemize}
    \item $K_t^{*\top} S_t K_t^* \rightarrow$ \texttt{term\_quad} (The quadratic reduction)
    \item $K_t^{*\top} S_t \sigma_t^* \rightarrow$ \texttt{term\_lin} (The linear reduction)
    \item $P_t \rightarrow$ \texttt{P} (updated for next loop iteration)
    \item $p_t \rightarrow$ \texttt{p} (updated for next loop iteration)
\end{itemize}

\subsection{Phase 3: Update (Closed-Loop Rollout)}
We generate the new control sequence. Crucially, we employ a \textbf{Closed-Loop} update rule. This means we apply the feedforward correction $\sigma_t^*$ scaled by a step size $\alpha$, plus the feedback reaction to the \textit{actual} state deviation experienced during the new simulation.
\begin{equation}
    u_{new}(t) = \bar{u}_t + \alpha \cdot \sigma_t^* + K_t^* \cdot (x_{new}(t) - \bar{x}_t)
\end{equation}
\textbf{Code Mapping:}
\begin{itemize}
    \item $\alpha \rightarrow$ \texttt{alpha} (Determined via Armijo Line Search)
    \item $x_{new}(t) - \bar{x}_t \rightarrow$ \texttt{dx\_deviation}
    \item Update $\rightarrow$ \texttt{u\_new[i] = u[i] + du}
\end{itemize}
The feedback term $K_t^*$ stabilizes the unstable Acrobot dynamics during the rollout, ensuring that the new trajectory remains valid even when moving far from the equilibrium.

















\chapter{Task 3: Trajectory Tracking via LQR}

\section{Linearization around the Optimal Generated Trajectory}The optimal trajectory $(x^*_t, u^*_t)$ generated in the previous chapter serves as the nominal path \cite{Notarstefano2025Trajectory}. To handle disturbances and model uncertainties, we linearize the discrete-time dynamics $x_{t+1} = f_d(x_t, u_t)$ by performing a first-order Taylor expansion about this optimal nominal trajectory.
This produces a time-varying linear approximation of the system that is locally valid for small deviations from the reference trajectory:\begin{equation}\Delta x_{t+1} \approx A_t^{\text{traj}} \Delta x_t + B_t^{\text{traj}} \Delta u_t \quad t = 0, \ldots, T-1\end{equation}where $\Delta x_t = x_t - x^*_t$ and $\Delta u_t = u_t - u^*_t$. In this formulation, the Jacobian matrices $A_t^{\text{traj}}$ and $B_t^{\text{traj}}$ 
represent the system's sensitivity to small perturbations from the planned path, and are defined as:

\begin{equation}
    A_t^{\text{traj}} = \frac{\partial f_d}{\partial x} \Big|_{x^*_t, u^*_t}, \quad B_t^{\text{traj}} = \frac{\partial f_d}{\partial u} \Big|_{x^*_t, u^*_t}
\end{equation}

By updating these matrices at every time step $t$, the controller accounts for the changing geometry and velocity of the Acrobot, effectively "straightening" the nonlinear physics into a sequence of linear sub-problems that can be solved efficiently.

\section{Discrete-Time Finite-Horizon TV-LQR Optimization}
The core of the tracking problem is formulated as an optimization task over a finite time horizon $T$. Given the nominal trajectory $(x^*_t, u^*_t)$, we define the tracking error states $\Delta x_t = x_t - x^*_t$ and control deviations $\Delta u_t = u_t - u^*_t$. The goal is to find an optimal sequence of control deviations that minimizes a quadratic cost function:
\begin{equation}
    \sum_{t=0}^{T-1} (\Delta x_t^\top Q_{\text{reg}} \Delta x_t + \Delta u_t^\top R_{\text{reg}} \Delta u_t) + \Delta x_T^\top Q_{\text{final}} \Delta x_T
\end{equation}
subject to: $\Delta x_{t+1} = A_t^\text{traj} \Delta x_t + B_t^\text{traj} \Delta u_t$ \quad t=0,\ldots,T-1 \qquad $\Delta x_{0} = 0$

\quad

The solution to this problem is obtained via dynamic programming, specifically by solving the discrete-time Riccati difference equations backwards in time. This process yields a sequence of optimal cost-to-go matrices $P_t$, which are used to compute the feedback gains.

\section{Implementation of the Time-Varying Feedback Controller}
A crucial aspect of our implementation is the separation between the optimization phase (computing gains) and the control phase (applying the feedback law).

\subsection{Phase 1: Computation of Optimal Feedback Gains}
Before starting the tracking experiment, we perform a backward pass from $t = T$ down to $t = 0$. In this phase, we pre-compute and store the optimal feedback gains $K_t$ for every time step:
\begin{subequations}
    \begin{align}
        P_T &= Q_{\text{final}} \\
        K^{\text{reg}}_t &= -(R_{\text{reg}} + B_t^{\text{traj, T}} P_{t+1} B_t^{\text{traj}})^{-1} (B_t^{\text{traj, T}} P_{t+1} A_t^{\text{traj}}) \\
        P_t &= Q_{\text{reg}} + A_t^{\text{traj}, T}P_{t+1} A_t^{\text{traj}} + (A_t^{\text{traj}, T}P_{t+1} B_t^{\text{traj}}) K^{\text{reg}}_t
    \end{align}
\end{subequations}
This pre-computation ensures that the controller can operate in real-time during the forward simulation, as the heavy matrix operations are already solved.

\subsection{Phase 2: State Feedback Control Law}
Once the gains $K_t$ are available, the physical system is controlled using a feedback controller. At each sampling instant $t$, the controller measures the actual state $x_t$, computes the error $\Delta x_t$, and applies the following control command:
\quad

\begin{equation}
    u_t = u^{traj}_t + K^{reg}_t (x_t - x^{traj}_t)
\end{equation}

\begin{equation}
    x_{t+1} = f_d(x_t, u_t), \quad t=0,\ldots,T-1
\end{equation}
Here, $u^{traj}_t$ acts as the \textit{feedforward} nominal torque required to maintain the ideal trajectory, while the term $ K_t \Delta x_t$ provides the \textit{feedback} corrective action, after applying the corrective action, we compute the new state $x_{t+1}$ which will be more close to the desired trajectory. This dual structure allows the Acrobot to stay close to the optimized swing-up path even when faced with initial condition perturbations or unmodeled dynamics, leveraging the local optimality of the LQR design.
	









\chapter{Task 4: Trajectory Tracking via MPC}

While Task 3 focused on a pre-computed, offline feedback law (LQR), Task 4 introduces \textbf{Model Predictive Control (MPC)} for trajectory tracking \cite{Notarstefano2025MPC}. The fundamental difference lies in the fact that MPC does not rely on a fixed sequence of gains. Instead, at every single time step $t$, it solves an \textbf{online optimization problem} to find the best possible control action. This receding-horizon approach makes MPC more robust and flexible than a standard LQR tracker, particularly for large deviations from the nominal trajectory.


\section{The Receding Horizon Strategy}
The implemented MPC controller employs a \textbf{Linear Time-Varying (LTV)} formulation with a receding horizon strategy. At each time step $t$, the controller plans over a finite prediction horizon of $T$ steps into the future, but only executes the very first control command. This process is then repeated at the next sampling instant, incorporating new state information.

This principle allows the controller to continuously integrate new measurements at every sampling instance, providing a natural feedback mechanism that compensates for errors introduced by the first-order Taylor linearization of the Acrobot's nonlinear dynamics.


\section{MPC Problem Formulation}
At each time step $t$, given the measured state $x_t^{\text{meas}}$, the MPC controller solves the following finite-horizon optimal control problem:

\begin{equation}
    \min_{\Delta x, \Delta u} \sum_{k=0}^{T-1} \left( \Delta x_{k|t}^\top Q_{\text{mpc}} \Delta x_{k|t} + \Delta u_{k|t}^\top R_{\text{mpc}} \Delta u_{k|t} \right) + \Delta x_{T|t}^\top P_{\text{mpc}} \Delta x_{T|t}
\end{equation}
subject to the linearized dynamics constraints:
\begin{equation}
    \Delta x_{k+1|t} = A_{t+k} \Delta x_{k|t} + B_{t+k} \Delta u_{k|t}, \quad k = 0, \ldots, T-1
\end{equation}
and the initial condition constraint:
\begin{equation}
    \Delta x_{0|t} = x_t^{\text{meas}} - x^*_t
\end{equation}

Here, $\Delta x_{k|t}$ and $\Delta u_{k|t}$ represent the predicted state and control deviations from the reference trajectory at time $t+k$, as predicted at time $t$. The notation $k|t$ emphasizes the predictive nature of MPC.


\section{Terminal Cost Design for Stability}
A critical component of the MPC formulation is the terminal cost matrix $P_{\text{mpc}}$. To ensure closed-loop stability, this matrix is obtained by solving the \textbf{Discrete Algebraic Riccati Equation (DARE)} at the target equilibrium $x_{eq2}$:

\begin{equation}
    P_{\text{mpc}} = A^\top P_{\infty} A - A^\top P_{\infty} B (R + B^\top P_{\infty} B)^{-1} B^\top P_{\infty} A + Q
\end{equation}

The resulting matrix $P_{\text{mpc}} = P_{\infty}$ represents an infinite-horizon cost-to-go at the terminal state. From a theoretical perspective, this terminal cost acts as a Lyapunov function, providing guarantees of closed-loop stability for the MPC controller when combined with the finite horizon.


\section{Local Linearization within the Horizon}
Since the Acrobot is a highly nonlinear system, the MPC controller performs local linearization at each step of the prediction horizon. The Jacobian matrices $A_{t+k}$ and $B_{t+k}$ are computed using finite differences about the reference trajectory points $(x^*_{t+k}, u^*_{t+k})$:
\begin{equation}
    A_{t+k} \approx \frac{\partial f_d}{\partial x}\Big|_{(x^*_{t+k}, u^*_{t+k})}, \quad B_{t+k} \approx \frac{\partial f_d}{\partial u}\Big|_{(x^*_{t+k}, u^*_{t+k})}
\end{equation}

This time-varying linearization allows the MPC to capture the local physics of the system across the entire prediction horizon, adapting to the changing dynamics along the swing-up trajectory.


\section{Online Optimization via Quadratic Programming}
The optimization problem is structured as a \textbf{Quadratic Program (QP)} and solved at each time step using an efficient solver (OSQP). The QP structure consists of:

\begin{itemize}
    \item \textbf{Hessian Matrix $H$}: A large, sparse, block-diagonal matrix that aggregates all stage costs ($Q_{\text{mpc}}$, $R_{\text{mpc}}$) and the terminal cost ($P_{\text{mpc}}$) over the prediction horizon.
    \item \textbf{Equality Constraints}: The linearized dynamics $\Delta x_{k+1|t} = A_{t+k} \Delta x_{k|t} + B_{t+k} \Delta u_{k|t}$ are enforced as linear equality constraints, ensuring that the predicted trajectory remains dynamically consistent.
    \item \textbf{Initial Condition}: The optimization is constrained to start at the current measured deviation: $\Delta x_{0|t} = x_t^{\text{meas}} - x_{t,ref}$.
\end{itemize}

The QP solver returns the optimal sequence of control corrections $\{\Delta u^*_{t|t},\ldots, \Delta u^*_{t+T-1|t}\}$. Following the receding horizon principle, only the first correction $\Delta u^*_{t|t}$ is applied.


\section{Closed-Loop Simulation}
The MPC controller is tested in closed-loop simulation to evaluate its robustness against perturbations. At each time step $t$, the control loop executes the following steps:

\begin{enumerate}
    \item \textbf{Measurement}: Obtain the current simulated state $x_t^{\text{meas}}$.
    \item \textbf{Optimization}: Solve the MPC QP problem to compute $\Delta u^*_{t|t}$.
    \item \textbf{Actuation}: Apply the control input to the full nonlinear model:
    \begin{equation}
        u_{\text{applied}} = u^*_t + \Delta u^*_{t|t}
    \end{equation}
    \item \textbf{Integration}: Compute the next state using the nonlinear dynamics:
    \begin{equation}
        x_{t+1} = f_d(x_t^{\text{meas}}, u_{\text{applied}})
    \end{equation}
\end{enumerate}

By continuously re-optimizing and incorporating feedback, the MPC controller successfully rejects large perturbations and ensures swing-up and stabilization of the Acrobot at the upright equilibrium.





\chapter{Task 5: Animation and Results}
\label{chap:results}

In this chapter, we present the numerical results for all tasks, including trajectory generation performance and tracking effectiveness under various conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task 1: Trajectory Generation (Step Reference)}
\label{sec:task1_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Performance metrics for the iLQR algorithm using a simple step reference.

\subsection{Trajectories and Convergence Metrics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_opt_des_traj.png}
    \caption{Task 1: Optimal vs Desired Trajectories.}
    \label{fig:task1_opt_des}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_intermediate.png}
    \caption{Task 1: Intermediate Trajectories during optimization.}
    \label{fig:task1_intermediate}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_norm_descent.png}
    \caption{Task 1: Norm of Descent Direction (Semi-Logarithmic Scale).}
    \label{fig:task1_norm_descent}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_cost_evolution.png}
    \caption{Task 1: Cost Evolution (Semi-Logarithmic Scale).}
    \label{fig:task1_cost_evolution}
\end{figure}

\subsection{Armijo Landscapes Across Iterations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_1.png}
    \caption{Task 1: Armijo Landscape (Iteration 0).}
    \label{fig:task1_armijo_0}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_2.png}
    \caption{Task 1: Armijo Landscape (Iteration 1).}
    \label{fig:task1_armijo_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_3.png}
    \caption{Task 1: Armijo Landscape (Iteration 10).}
    \label{fig:task1_armijo_10}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task1_6.png}
    \caption{Task 1: Armijo Landscape (Iteration 40).}
    \label{fig:task1_armijo_40}
\end{figure}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task 2: Trajectory Generation (Smooth Reference)}
\label{sec:task2_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Comparison of the iLQR performance when initiated with a physically-inspired smooth reference.

\subsection{Trajectories and Convergence Metrics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_opt_des_traj.png}
    \caption{Task 2: Optimal vs Desired Trajectories.}
    \label{fig:task2_opt_des}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_intermediate.png}
    \caption{Task 2: Intermediate Trajectories during optimization.}
    \label{fig:task2_intermediate}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_norm_descent.png}
    \caption{Task 2: Norm of Descent Direction (Semi-Logarithmic Scale).}
    \label{fig:task2_norm_descent}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_cost_evolution.png}
    \caption{Task 2: Cost Evolution (Semi-Logarithmic Scale).}
    \label{fig:task2_cost_evolution}
\end{figure}

\subsection{Armijo Landscapes Across Iterations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_1.png}
    \caption{Task 2: Armijo Landscape (Iteration 0).}
    \label{fig:task2_armijo_0}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_2.png}
    \caption{Task 2: Armijo Landscape (Iteration 1).}
    \label{fig:task2_armijo_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_3.png}
    \caption{Task 2: Armijo Landscape (Iteration 10).}
    \label{fig:task2_armijo_10}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task2_6.png}
    \caption{Task 2: Armijo Landscape (Iteration 40).}
    \label{fig:task2_armijo_40}
\end{figure}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task 3: LQR Tracking Performance}
\label{sec:task3_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tracking capability of the Time-Varying LQR controller under different initial condition perturbations.

\subsection{System Trajectory vs Desired Optimal Trajectory}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_1_performance.png}
    \caption{Task 3: System Trajectory vs Optimal Trajectory (Perturbation 1).}
    \label{fig:task3_traj_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_2_performance.png}
    \caption{Task 3: System Trajectory vs Optimal Trajectory (Perturbation 2).}
    \label{fig:task3_traj_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_3_performance.png}
    \caption{Task 3: System Trajectory vs Optimal Trajectory (Perturbation 3).}
    \label{fig:task3_traj_3}
\end{figure}

\subsection{Tracking Error}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_1_errors.png}
    \caption{Task 3: Tracking Error (Perturbation 1).}
    \label{fig:task3_err_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_2_errors.png}
    \caption{Task 3: Tracking Error (Perturbation 2).}
    \label{fig:task3_err_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task3_3_errors.png}
    \caption{Task 3: Tracking Error (Perturbation 3).}
    \label{fig:task3_err_3}
\end{figure}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task 4: MPC Tracking Performance}
\label{sec:task4_results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tracking capability of the Model Predictive Controller under different initial condition perturbations.

\subsection{System Trajectory vs Desired Optimal Trajectory}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_1_performance.png}
    \caption{Task 4: System Trajectory vs Optimal Trajectory (Perturbation 1).}
    \label{fig:task4_traj_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_2_performance.png}
    \caption{Task 4: System Trajectory vs Optimal Trajectory (Perturbation 2).}
    \label{fig:task4_traj_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_3_performance.png}
    \caption{Task 4: System Trajectory vs Optimal Trajectory (Perturbation 3).}
    \label{fig:task4_traj_3}
\end{figure}

\subsection{Tracking Error}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_1_errors.png}
    \caption{Task 4: Tracking Error (Perturbation 1).}
    \label{fig:task4_err_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_2_errors.png}
    \caption{Task 4: Tracking Error (Perturbation 2).}
    \label{fig:task4_err_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/output_task4_3_errors.png}
    \caption{Task 4: Tracking Error (Perturbation 3).}
    \label{fig:task4_err_3}
\end{figure}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Animation}

The following sequence illustrates the Acrobot's swing-up maneuver as computed by the trajectory generation algorithm. Starting from the stable downward equilibrium (left), the robot pumps energy through coordinated hip torques, reaching a mid-swing configuration (center), and finally stabilizes at the unstable inverted equilibrium (right). This demonstrates the successful execution of the optimal control strategy developed throughout this project.

\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/acrobot_first_frame.png}
        \subcaption{Initial position ($t=0$)}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/acrobot_5s_frame.png}
        \subcaption{Mid-swing ($t=5$ s)}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/acrobot_last_frame.png}
        \subcaption{Final position ($t=T$)}
    \end{minipage}
    \caption{Acrobot swing-up animation sequence: from stable hanging to inverted equilibrium.}
    \label{fig:acrobot_animation}
\end{figure}

Note that the animation is stored in gif format in the figs folder.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%% Conclusions %%%%%%%%%%
	\chapter*{Conclusions}
	\addcontentsline{toc}{chapter}{Conclusions} 
	
This project has successfully demonstrated a complete optimal control framework for the swing-up and stabilization of the Acrobot, an underactuated nonlinear robotic system. The development progressed through four interconnected phases, each building upon the previous to achieve the final objective.

\section*{The Role of Discretization}
The accurate discretization of the continuous-time dynamics proved to be a fundamental prerequisite for the entire project. By employing a 4th-order Runge-Kutta (RK4) scheme with a sufficiently small sampling time ($d_t = 0.01$ s), we ensured that the discrete-time model faithfully captures the nonlinear behavior of the Acrobot, including the Coriolis, centrifugal, and gravitational effects. This high-fidelity discretization is essential not only for realistic forward simulation but also for obtaining accurate Jacobian matrices ($A_t$, $B_t$) via finite differences. A poor discretization would have introduced numerical errors that propagate through the optimization and control stages, leading to suboptimal or even divergent solutions.

\section*{Trajectory Generation: iLQR as an Efficient Planning Strategy}
For the trajectory generation phase (Tasks 1 and 2), we adopted the iterative Linear Quadratic Regulator (iLQR) algorithm, which can be viewed as a computationally efficient variant of Differential Dynamic Programming (DDP). While DDP forms a second-order approximation of the value function and involves computing Hessians of the dynamics, iLQR simplifies the problem by neglecting the second-order terms---specifically the products of Hessians with the cost-to-go---and relies solely on first-order (Jacobian) linearizations. This simplification results in significant computational savings without a substantial loss in convergence speed for many practical systems.

The essence of iLQR lies in its dynamic programming structure: at each iteration, a backward pass solves a sequence of local LQR subproblems to compute optimal feedback gains ($K_t$) and feedforward corrections ($\sigma_t$), followed by a forward pass that simulates the system with the updated control law. This approach efficiently generates a feasible and locally optimal trajectory, serving as the \textit{planning phase} of the control problem. The choice of reference trajectory (step vs. smooth) significantly affected convergence, highlighting the importance of providing the optimizer with a physically consistent initial guess.



It is important to note that this task could have been performed using other types of closed-loop optimization algorithms.Ultimately, we chose this approach to establish a direct connection with Task 3. In terms of implementation, it is crucial to recognize that the feedback gain $K_T$ from the final iteration of the iLQR (used for trajectory generation in Task 2) will be identical to the optimal gain 
$K_{\text{reg}}^*$ obtained when solving the standard LQR for trajectory tracking. This mathematical equivalence occurs because, in both cases, the linearization of the nonlinear dynamics is performed around the exact same optimal reference trajectory.

\section*{Trajectory Tracking: Offline LQR Design for Real-Time Execution}
The tracking problem (Task 3) fundamentally differs from the generation problem in its objective and structure. Once the optimal trajectory $(x^*_t, u^*_t)$ has been generated, the goal shifts to \textit{maintaining} the system on this reference path despite perturbations and model uncertainties. We formulated this as a standard Time-Varying LQR problem with the following key characteristics:

\begin{itemize}
    \item \textbf{Linearization Over the Optimal Trajectory:} The system dynamics are linearized around the pre-computed optimal trajectory (not around equilibrium points), yielding time-varying Jacobians $A_t^{\text{traj}}$ and $B_t^{\text{traj}}$.
    \item \textbf{Incremental Formulation:} The optimization variables are the state and control \textit{deviations} from the nominal path ($\Delta x_t$, $\Delta u_t$), not the absolute states.
    \item \textbf{Quadratic Cost, Linear Constraints:} The resulting problem is a standard LQR with a quadratic objective and linear dynamics, which admits a closed-form solution via the Riccati difference equations.
    \item \textbf{One-Shot Offline Computation:} Unlike iLQR which iterates until convergence, the tracking LQR requires only a \textit{single backward pass} to compute all feedback gains $K_t^{\text{reg}}$. This is because the problem is already linear-quadratic by construction—there is no need for iterative linearization.
\end{itemize}

This offline pre-computation of the gains enables real-time control execution: during operation, the controller simply reads the current state, computes the deviation, and applies the pre-stored gain to generate the corrective input with minimal computational overhead.

\section*{LQR vs. MPC: A Comparative Perspective}
Both the Time-Varying LQR and Model Predictive Control (MPC) are trajectory-tracking strategies, but they differ significantly in philosophy and implementation:

\begin{table}[H]
    \centering
    \begin{tabular}{l p{5.5cm} p{3.5cm}}
        \hline
        \textbf{Aspect} & \textbf{LQR Tracking} & \textbf{MPC Tracking} \\
        \hline
        \textbf{Computation} & All gains computed \textit{offline} in a single backward pass. & Optimization solved \textit{online} at each time step. \\
        \textbf{Constraint Handling} & Cannot explicitly handle state/input constraints (relies on linear assumptions). & Naturally incorporates inequality constraints (e.g., torque limits, joint angle limits). \\
        \textbf{Receding Horizon} & Uses a fixed terminal time $T$ matching the trajectory length. & Uses a rolling prediction horizon that advances with time. \\
        \textbf{Real-Time Feasibility} & Extremely fast execution (simple matrix multiplication). & Computationally heavier; requires efficient solvers for real-time use. \\
        \textbf{Robustness} & Degrades for large perturbations (linear approximation breaks down). & Re-optimizes at each step, adapting to the current state more robustly. \\
        \hline
    \end{tabular}
    \caption{Comparison between LQR and MPC for Trajectory Tracking}
    \label{tab:lqr_vs_mpc}
\end{table}

In essence, LQR is optimal for small perturbations and when computational resources are limited, while MPC offers greater flexibility and robustness at the cost of increased online computation. For the Acrobot swing-up task, both approaches successfully tracked the reference trajectory, but MPC demonstrated superior performance under larger initial perturbations where the LQR's linear assumptions become less accurate.


	%%%%%%%%%% Bibliography %%%%%%%%%%%
	% Ensure bibfile.bib exists or use biblatex
	\addcontentsline{toc}{chapter}{Bibliography}
	\bibliographystyle{plain}
	\bibliography{bibfile}
	
\end{document}